{
	"nodes":[
		{"id":"36ebc6db57aed5b0","type":"text","text":"# название проекта - \"NG-IDS\"\n","x":-156,"y":-60,"width":313,"height":100},
		{"id":"29298eb29c2c89d7","type":"text","text":"# Выбор языков\n\n## Rust\n- безопасная работа с памятью\n- сниффинг\n- анализ пакетов\n- парсинг пакетов\n- фильтрация пакетов\n- предобработка данных\n- высокая производительность сравнимая с C\n## Python\n- прототипирование нейросетей\n- прототипирование алгоритмов\n- Большой выбор библиотек для машинного обучения\n","x":-2080,"y":883,"width":410,"height":515},
		{"id":"4034e1d4de8cdfb0","type":"text","text":"# библиотеки/зависимости\n\n## Rust\n1. **pcap**\n\tнеобходим для захвата и анализа пакетов\n2. **serde**\n\tдля сериализации и десериализации данных в различных форматах\n4. **tokio**\n\tдля асинхронности производительность\n## Python\n1. **keras**\n\tпрототип нейросетей/интерфейс\n2. **tensorflow**\n\tпроектирования, тренировки и развертывания моделей машинного обучения\n3. **scikit-learn**\n\tалгоритмов классификации, регрессии и кластеризации\n4. **etherparse**\n\tдекодирования сетевых протоколов, работа на \"высоких\" протоколах\n","x":-1525,"y":883,"width":440,"height":600},
		{"id":"4cd5e0b80bdd4d4d","type":"text","text":"# примененение и выбор технологий \n## библиотеки\n\nих эффективность в примененение и целесообразность, \n## выбор языков\n\nоптимальных по безопасности, производительности и возможностях,\n## технологии для съёма трафика\nэффективные в коммерчерских и производительных планах\n","x":-1480,"y":280,"width":350,"height":480},
		{"id":"df256b209dd24b74","type":"text","text":"# технологии для захвата трафика\n\n## SPAN-порты\n\nИспользование SPAN-портов (Switched Port Analyzer) в технологии IDS (система обнаружения вторжений) предоставляет значительные преимущества, включая возможность перехвата и анализа трафика в реальном времени без воздействия на производительность активной сети. Это позволяет системе IDS мониторить все сетевые пакеты, проходящие через коммутатор, обеспечивая точное и всестороннее обнаружение потенциальных угроз и аномалий. Кроме того, SPAN-порты облегчают развертывание IDS, так как не требуют физического вмешательства в сетевую инфраструктуру и позволяют гибко выбирать интересующие сегменты трафика для анализа, упрощая настройку и поддержку системы безопасности.","x":-940,"y":883,"width":350,"height":600},
		{"id":"33b549daa63efa34","type":"text","text":"# точность и скорость нейросетей в анализе сетевого трафика в реальном времени\n","x":-170,"y":274,"width":345,"height":153},
		{"id":"b2ee45b1a8efc009","type":"text","text":"# Обучающая выборка\n\nДля достижения высокой произвопосле подключения второго монитора дительности и точности, критически важно разработать качественную обучающую выборку, которая учитывает **многофакторный анализ** сетевых данных. \n\nТщательный анализ и статистическая обработка наиболее значимых параметров обнаружения вторжений позволяют определить, какие характеристики трафика наиболее информативны и должны быть включены в обучающий набор данных, тем самым сокращая объём данных без потери критически важной информации. Это позволяет за счёт небольшой потери точности выявления вторжений, значительно ускорить процесс обработки данных нейросетью, оптимизируя её работу в условиях реального времени.\n\nТакой подход обеспечивает баланс между точностью обнаружения и требованиями к скорости обработки, что является ключевым для эффективной эксплуатации системы мониторинга защищённости.","x":-307,"y":883,"width":620,"height":400},
		{"id":"74a171b281868feb","type":"text","text":"# настройка нейросети","x":800,"y":-140,"width":250,"height":130},
		{"id":"484754f6f5c56310","type":"text","text":"# автоэнкодер","x":1200,"y":120,"width":250,"height":180},
		{"id":"5ff57499aa047783","type":"text","text":"# LSTM","x":1200,"y":-320,"width":250,"height":180},
		{"id":"77b37d1047dda455","type":"text","text":"# Оптимизация ADAM в обучении\n\nОптимизация алгоритма Adam для уменьшения потребления производительных ресурсов важна, особенно когда используется в сочетании с ресурсоемкими моделями, такими как LSTM. Вот несколько стратегий, которые можно использовать для оптимизации Adam и снижения его влияния на производительность:\n\n### 1. **Уменьшение размера мини-пакета**\nИспользование меньших мини-пакетов данных при обучении может уменьшить потребление памяти и ускорить процесс обновления весов, хотя это также может повлиять на стабильность и скорость сходимости обучения. Экспериментирование с размером мини-пакета поможет найти баланс между производительностью и эффективностью обучения.\n\n### 2. **Снижение точности данных**\nПереход на более низкую точность чисел, например, с float64 (двойная точность) на float32 или даже float16 (полупрецизионные форматы), может значительно снизить потребление памяти и ускорить вычисления, особенно на GPU, поддерживающих оптимизации для float16.\n\n### 3. **Оптимизация вычислений**\nиспользовать правильные библиотеки для выполнения математических операций (например, использование CuDNN для операций на NVIDIA GPU). Это может существенно ускорить операции с тензорами, используемыми в Adam.\n\n### 4. **Разреженное обновление градиентов**\nДля задач, где градиенты часто содержат много нулей (разреженные данные), можно использовать разреженные представления данных и обновлять только те веса, которые имеют ненулевые градиенты. Это может существенно сократить количество вычислений и использование памяти.\n\n### 5. **Периодическое обновление параметров**\nВместо обновления параметров модели после каждого мини-пакета можно обновлять их после нескольких мини-пакетов или даже после полного прохода по данным (epoch). Это может увеличить время обучения, но уменьшить вычислительную нагрузку в каждом отдельном случае обновления.\n\n### 6. **Тонкая настройка гиперпараметров**\nТщательно подбирать гиперпараметры Adam, такие как скорость обучения и параметры распада моментов, чтобы максимизировать эффективность обучения и минимизировать количество эпох, необходимых для сходимости.\n\nЭти стратегии могут помочь оптимизировать использование Adam при обучении LSTM, снижая требования к вычислительным мощностям и улучшая общую эффективность системы.","x":-3890,"y":-1631,"width":690,"height":1231},
		{"id":"274cb7907201fc8f","type":"text","text":"# Решение\n\nПервый уровень (автоэнкодер, обозначенный как N1) и второй уровень (LSTM, обозначенный как N2) работают взаимосвязанно, обеспечивая динамическое обучение и корректировку модели на основе реальных данных.\n\nN1 отвечает за первичное обнаружение аномалий в трафике. Если N1 классифицирует трафик как аномальный, он передает данные на второй уровень N2 для дальнейшей проверки. N2 анализирует эти данные и, основываясь на более глубоком анализе, подтверждает или опровергает вердикт N1. В случае если N2 считает пакет нормальным, он информирует об этом N1, который использует эту информацию для корректировки своих параметров.\n\nПроцесс переобучения модели N1 осуществляется на основе соотношения между количеством ложных срабатываний и верными детектированиями аномалий. Обозначим это соотношение как \\(y\\). Параметр \\(x\\) задает пороговое значение, при превышении которого считается, что модель требует переобучения на новых данных. Формула переобучения выглядит следующим образом:\n\n$N2(y) < x$\n\nгде $(N2(y))$ — функция оценки соотношения ложных к истинным от $N2$.\n\n$N1$ может переобучаться на данных, поступающих в реальном времени, или на данных, предварительно подготовленных условно за неделю до текущего момента. Этот процесс повторяется до тех пор, пока количество ложных срабатываний не стабилизируется и не будет на 10% выше, чем соотношение ложных срабатываний к нормальному трафику в реальном времени.\n\nТаким образом, данная система моделирует обучение с учителем, где $N2$ и эксперт SOC выступаюют в роли \"учителей\" для $N1$, обеспечивая непрерывную адаптацию и оптимизацию процесса обнаружения аномалий на основе текущей сетевой активности.\n\n","x":-4370,"y":-360,"width":590,"height":798},
		{"id":"bebebfb7b1008d28","type":"text","text":"# Проблема адаптивности нормали трафика\n\nс течением времени сетевой трафик может выходить за пределы нормального трафика слишком часто. то ли это будет зависеть от расширения, смены провайдера, использования специфичных ресурсов и нейросеть необходимо к этому подготовить, т.к. от этого зависит: количество ложных срабатываний, нагрузка на второй уровень - LSTM и эффективность обнаружения вторжений.","x":-3570,"y":-140,"width":370,"height":358},
		{"id":"15bcc56692d3fa3e","type":"text","text":"# Автоэнкодер\n\nАвтоэнкодеры — это тип искусственных нейронных сетей, используемых для обучения эффективного кодирования неотмеченных данных. Процесс обучения автоэнкодера направлен на изучение представления (кодирования) входных данных в сжатом, но информативном виде. Вот как происходит обучение автоэнкодера:\n\n### Этап 1: Архитектура сети\nАвтоэнкодер обычно состоит из двух основных частей:\n- **Энкодер**: Эта часть сети принимает входные данные и преобразует их в более компактное внутреннее представление, называемое \"кодированным\" или \"скрытым\" слоем. Энкодер обычно содержит несколько слоев, уменьшающих размерность данных на каждом шаге.\n- **Декодер**: Декодер работает наоборот, он принимает кодированное представление и пытается восстановить исходные данные. Обычно декодер зеркально отражает структуру энкодера, постепенно увеличивая размерность данных до исходной.\n\n### Этап 2: Функция потерь\nЦентральным элементом обучения автоэнкодера является функция потерь, которая измеряет ошибки между исходными входными данными и их восстановленными версиями, полученными после прохождения через энкодер и декодер. Обычно это среднеквадратичное отклонение или бинарная кросс-энтропия, в зависимости от типа и характеристик входных данных.\n\n### Этап 3: Оптимизация\nПроцесс обучения заключается в минимизации функции потерь путем подстройки весов сети. Для этого обычно используется алгоритм обратного распространения ошибок в сочетании с оптимизатором, таким как SGD, Adam и другие. В процессе обучения энкодер и декодер учатся сжимать данные и восстанавливать их таким образом, чтобы максимально точно воспроизвести исходный ввод.\n\n### Этап 4: Тренировочные данные\nАвтоэнкодеры обучаются на больших наборах данных без меток. Это означает, что для обучения не требуются размеченные примеры, что делает автоэнкодеры особенно полезными для работы с неразмеченными данными.\n\n### Этап 5: Регуляризация\nЧтобы избежать переобучения и улучшить обобщающую способность модели, в архитектуру автоэнкодера часто включают элементы регуляризации. Один из распространенных методов — это добавление \"шума\" к входным данным или использование \"Dropout\", где случайные нейроны временно исключаются из процесса обучения на каждом шаге.\n\n### Этап 6: Валидация и тестирование\nПосле тренировки модель тестируется на новых данных, чтобы оценить, насколько хорошо она может обобщать на невиданные примеры. Это важный шаг для проверки эффективности автоэнкодера","x":-1875,"y":-1080,"width":875,"height":1250},
		{"id":"d4f04f432cbc3f45","type":"text","text":"# Обучение\n\nAdam — это усовершенствованная версия SGD, он адаптивно корректирует величину изменения весов для каждого параметра модели, что помогает более эффективно находить минимальное значение функции потерь. является одним из самых популярных алгоритмов оптимизации в области машинного обучения, особенно при работе с нейронными сетями, такими как автоэнкодеры. Этот метод оптимизации сочетает в себе лучшие качества алгоритмов AdaGrad и RMSProp, предлагая эффективное и универсальное решение для обучения моделей.\n\n### Плюсы использования Adam в обучении автоэнкодера:\n\n1. **Адаптивная скорость обучения:**\n   - Adam автоматически настраивает скорость обучения для каждого параметра на основе оценок первого и второго моментов градиентов. Это позволяет алгоритму быть менее чувствительным к выбору исходной скорости обучения и к различиям в масштабах данных или параметров.\n\n2. **Эффективность на разнообразных задачах:**\n   - Adam показал свою эффективность на широком спектре задач в глубоком обучении, включая задачи, где пространства параметров большие и сложные, как в автоэнкодерах для обработки изображений или сложных временных последовательностей.\n\n3. **Быстрая сходимость:**\n   - Благодаря эффективному масштабированию шага градиента, Adam часто достигает сходимости быстрее, чем многие другие оптимизаторы, что делает его идеальным выбором для обучения моделей, требующих быстрого прототипирования и разработки.\n\n4. **Устойчивость к застреванию в локальных минимумах:**\n   - Adam лучше справляется с локальными минимумами, чем традиционный SGD, благодаря своей адаптивной природе.\n\n### Минусы использования Adam в обучении автоэнкодера:\n\n1. **Вычислительные затраты:**\n   - Adam требует хранения оценок моментов для каждого параметра, что увеличивает его память и вычислительные затраты.\n\n2. **Возможные проблемы с обобщением:**\n   - Некоторые исследования показывают, что модели, обученные с использованием Adam, могут страдать от худшего обобщения на тестовых данных по сравнению с теми, которые были обучены с использованием некоторых других методов оптимизации, особенно в задачах с очень шумными или разреженными данными.\n\n3. **Чувствительность к настройке гиперпараметров:**\n   - Хотя Adam и уменьшает необходимость в детальной настройке начальной скорости обучения, выбор более тонких гиперпараметров, таких как параметры распада моментов, всё ещё может значительно повлиять на производительность обучения.\n\n### Заключение\n\nAdam является мощным и удобным инструментом для обучения автоэнкодеров и других типов нейронных сетей, предлагая хорошее сочетание скорости и удобства использования. Однако, как и любой другой метод, он имеет свои ограничения и может требовать тщательной настройки в специфических случаях. Важно проводить эксперименты с различными оптимизаторами и настройками гиперпараметров для конкретных задач и данных, чтобы определить наилучший выбор.","x":-2890,"y":-985,"width":810,"height":1060},
		{"id":"a03aedb994397dee","x":320,"y":-525,"width":322,"height":70,"type":"text","text":"# Тестирование и валидация"},
		{"id":"d5b8dd7032fc4597","x":270,"y":-842,"width":322,"height":125,"type":"text","text":"# Работа с несбалансированными данными"},
		{"id":"5e47321ae164448b","x":431,"y":-355,"width":322,"height":125,"type":"text","text":"# Интеграция и автоматизация"},
		{"id":"3aea9110186beff8","type":"text","text":"# обучение нейросети\n\n","x":-702,"y":-500,"width":322,"height":90},
		{"id":"1aad854b233df8b0","type":"text","text":"# предобработка данных для нейросетей.","x":-745,"y":-60,"width":310,"height":100},
		{"id":"18631922dc48a78c","type":"text","text":"# LSTM","x":-1260,"y":-1400,"width":260,"height":120}
	],
	"edges":[
		{"id":"ec1de2919abb96ab","fromNode":"4cd5e0b80bdd4d4d","fromSide":"left","toNode":"29298eb29c2c89d7","toSide":"top"},
		{"id":"4a1f881b9d67fafb","fromNode":"4cd5e0b80bdd4d4d","fromSide":"bottom","toNode":"4034e1d4de8cdfb0","toSide":"top"},
		{"id":"75d056178f425f12","fromNode":"4cd5e0b80bdd4d4d","fromSide":"right","toNode":"df256b209dd24b74","toSide":"top"},
		{"id":"27e4bbd33823b177","fromNode":"36ebc6db57aed5b0","fromSide":"bottom","toNode":"4cd5e0b80bdd4d4d","toSide":"top"},
		{"id":"437bcfdeb3b9d715","fromNode":"36ebc6db57aed5b0","fromSide":"bottom","toNode":"33b549daa63efa34","toSide":"top"},
		{"id":"d0b3995516910e23","fromNode":"33b549daa63efa34","fromSide":"bottom","toNode":"b2ee45b1a8efc009","toSide":"top"},
		{"id":"42291bc0d508df76","fromNode":"36ebc6db57aed5b0","fromSide":"right","toNode":"74a171b281868feb","toSide":"left"},
		{"id":"9658c3ac46f7aece","fromNode":"3aea9110186beff8","fromSide":"left","toNode":"15bcc56692d3fa3e","toSide":"right"},
		{"id":"1ec66a98740cb2e0","fromNode":"74a171b281868feb","fromSide":"right","toNode":"5ff57499aa047783","toSide":"left"},
		{"id":"998f8f2fb6b26758","fromNode":"74a171b281868feb","fromSide":"right","toNode":"484754f6f5c56310","toSide":"left"},
		{"id":"52c569ac7a97e69a","fromNode":"bebebfb7b1008d28","fromSide":"left","toNode":"274cb7907201fc8f","toSide":"right"},
		{"id":"a12ea87dc84d04fb","fromNode":"15bcc56692d3fa3e","fromSide":"left","toNode":"d4f04f432cbc3f45","toSide":"right"},
		{"id":"b6ba2253faf9af02","fromNode":"d4f04f432cbc3f45","fromSide":"left","toNode":"bebebfb7b1008d28","toSide":"right"},
		{"id":"07c20b8ea1c84684","fromNode":"d4f04f432cbc3f45","fromSide":"left","toNode":"77b37d1047dda455","toSide":"right"},
		{"id":"7fa98a8f700337f5","fromNode":"36ebc6db57aed5b0","fromSide":"top","toNode":"a03aedb994397dee","toSide":"left"},
		{"id":"99607c40a3e326ca","fromNode":"36ebc6db57aed5b0","fromSide":"top","toNode":"d5b8dd7032fc4597","toSide":"left"},
		{"id":"987396d41f85e99a","fromNode":"3aea9110186beff8","fromSide":"left","toNode":"18631922dc48a78c","toSide":"right"},
		{"id":"e5a430941f693b45","fromNode":"36ebc6db57aed5b0","fromSide":"left","toNode":"1aad854b233df8b0","toSide":"right"},
		{"id":"f0359cc7efa1d51a","fromNode":"36ebc6db57aed5b0","fromSide":"right","toNode":"5e47321ae164448b","toSide":"left"},
		{"id":"eab217416f0e7a14","fromNode":"36ebc6db57aed5b0","fromSide":"left","toNode":"3aea9110186beff8","toSide":"right"}
	]
}